{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data pre-processing \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.corpus import stopwords \n",
    "#from sklearn.manifold import TSNE\n",
    "#from gensim.models import KeyedVectors\n",
    "import gensim.models\n",
    "from gensim.utils import lemmatize\n",
    "import nltk \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tag import pos_tag\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "\n",
    "# for classification modelling \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import string\n",
    "import sklearn.metrics as metrics\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset (news headlines and financial data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import news headlines\n",
    "df = pd.read_csv('finalData.csv')\n",
    "df = df.drop(['Unnamed: 2','Unnamed: 3'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import financial data\n",
    "# include column based on today's close to tomorrow's open (binary classification)\n",
    "stock_prices = pd.read_csv('AAPLfinal.csv')\n",
    "stock_prices = stock_prices.drop(['Unnamed: 0'], axis=1)\n",
    "stock_prices[\"Date\"] = pd.to_datetime(stock_prices[\"Date\"])\n",
    "\n",
    "for i in range(stock_prices.shape[0]-1):\n",
    "    if stock_prices['Close'][i]<stock_prices['Open'][i+1]:\n",
    "        stock_prices.loc[i,'response_variable']= 1\n",
    "    else:\n",
    "        stock_prices.loc[i,'response_variable']= -1\n",
    "\n",
    "\n",
    "stock_prices=stock_prices[:stock_prices.shape[0]-1] # remove the last row, no target class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj.Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Revenue</th>\n",
       "      <th>Changes.in.working.capital</th>\n",
       "      <th>Dividend.paid</th>\n",
       "      <th>Net.changes.in.cash</th>\n",
       "      <th>response_variable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>110.370003</td>\n",
       "      <td>110.940002</td>\n",
       "      <td>109.029999</td>\n",
       "      <td>109.489998</td>\n",
       "      <td>104.344177</td>\n",
       "      <td>37086900</td>\n",
       "      <td>46852</td>\n",
       "      <td>4127</td>\n",
       "      <td>-3902</td>\n",
       "      <td>2247</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-12-02</td>\n",
       "      <td>109.169998</td>\n",
       "      <td>110.089996</td>\n",
       "      <td>108.849998</td>\n",
       "      <td>109.900002</td>\n",
       "      <td>104.734901</td>\n",
       "      <td>26528000</td>\n",
       "      <td>46852</td>\n",
       "      <td>4127</td>\n",
       "      <td>-3902</td>\n",
       "      <td>2247</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-12-05</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>110.029999</td>\n",
       "      <td>108.250000</td>\n",
       "      <td>109.110001</td>\n",
       "      <td>103.982048</td>\n",
       "      <td>34324500</td>\n",
       "      <td>46852</td>\n",
       "      <td>4127</td>\n",
       "      <td>-3902</td>\n",
       "      <td>2247</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-12-06</td>\n",
       "      <td>109.500000</td>\n",
       "      <td>110.360001</td>\n",
       "      <td>109.190002</td>\n",
       "      <td>109.949997</td>\n",
       "      <td>104.782547</td>\n",
       "      <td>26195500</td>\n",
       "      <td>46852</td>\n",
       "      <td>4127</td>\n",
       "      <td>-3902</td>\n",
       "      <td>2247</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-12-07</td>\n",
       "      <td>109.260002</td>\n",
       "      <td>111.190002</td>\n",
       "      <td>109.160004</td>\n",
       "      <td>111.029999</td>\n",
       "      <td>105.811821</td>\n",
       "      <td>29998700</td>\n",
       "      <td>46852</td>\n",
       "      <td>4127</td>\n",
       "      <td>-3902</td>\n",
       "      <td>2247</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date        Open        High         Low       Close   Adj.Close  \\\n",
       "0 2016-12-01  110.370003  110.940002  109.029999  109.489998  104.344177   \n",
       "1 2016-12-02  109.169998  110.089996  108.849998  109.900002  104.734901   \n",
       "2 2016-12-05  110.000000  110.029999  108.250000  109.110001  103.982048   \n",
       "3 2016-12-06  109.500000  110.360001  109.190002  109.949997  104.782547   \n",
       "4 2016-12-07  109.260002  111.190002  109.160004  111.029999  105.811821   \n",
       "\n",
       "     Volume  Revenue  Changes.in.working.capital  Dividend.paid  \\\n",
       "0  37086900    46852                        4127          -3902   \n",
       "1  26528000    46852                        4127          -3902   \n",
       "2  34324500    46852                        4127          -3902   \n",
       "3  26195500    46852                        4127          -3902   \n",
       "4  29998700    46852                        4127          -3902   \n",
       "\n",
       "   Net.changes.in.cash  response_variable  \n",
       "0                 2247               -1.0  \n",
       "1                 2247                1.0  \n",
       "2                 2247                1.0  \n",
       "3                 2247               -1.0  \n",
       "4                 2247               -1.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stock_prices = stock_prices.drop(['Open','High','Low','Adj.Close'], axis=1)\n",
    "stock_prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection \n",
    "df_selection = stock_prices.copy()\n",
    "Y_selection = df_selection[\"response_variable\"].tolist() # response variable \n",
    "final_df_selection = df_selection.drop([\"response_variable\",\"Date\"], axis=1)\n",
    "X_selection = final_df_selection.values.tolist() # regressor variables\n",
    "X_selection, Y_selection = shuffle(X_selection, Y_selection, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run random forest model for feature selection\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_selection, Y_selection, test_size=0.2, random_state=42)\n",
    "rfc = RandomForestClassifier(random_state=32)\n",
    "rfc.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. feature Volume (0.175800)\n",
      "2. feature Open (0.153157)\n",
      "3. feature High (0.151942)\n",
      "4. feature Close (0.149763)\n",
      "5. feature Low (0.146649)\n",
      "6. feature Adj.Close (0.140836)\n",
      "7. feature Net.changes.in.cash (0.022347)\n",
      "8. feature Dividend.paid (0.020834)\n",
      "9. feature Revenue (0.019720)\n",
      "10. feature Changes.in.working.capital (0.018951)\n"
     ]
    }
   ],
   "source": [
    "# obtain features importance\n",
    "feature_importance = rfc.feature_importances_\n",
    "indices = np.argsort(feature_importance)[::-1]\n",
    "\n",
    "for f in range(len(stock_prices.columns)-2): # minus 2, one for response variable, one for date\n",
    "    print(\"%d. feature %s (%f)\" % (f + 1, stock_prices.columns[indices[f]+1], feature_importance[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Volume</th>\n",
       "      <th>response_variable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>110.370003</td>\n",
       "      <td>110.940002</td>\n",
       "      <td>37086900</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-12-02</td>\n",
       "      <td>109.169998</td>\n",
       "      <td>110.089996</td>\n",
       "      <td>26528000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-12-05</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>110.029999</td>\n",
       "      <td>34324500</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-12-06</td>\n",
       "      <td>109.500000</td>\n",
       "      <td>110.360001</td>\n",
       "      <td>26195500</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-12-07</td>\n",
       "      <td>109.260002</td>\n",
       "      <td>111.190002</td>\n",
       "      <td>29998700</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date        Open        High    Volume  response_variable\n",
       "0 2016-12-01  110.370003  110.940002  37086900               -1.0\n",
       "1 2016-12-02  109.169998  110.089996  26528000                1.0\n",
       "2 2016-12-05  110.000000  110.029999  34324500                1.0\n",
       "3 2016-12-06  109.500000  110.360001  26195500               -1.0\n",
       "4 2016-12-07  109.260002  111.190002  29998700               -1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select the top __ features based on the features importance\n",
    "number = 3 # number of features\n",
    "temp_stock_prices = stock_prices.copy()\n",
    "for i in range(number, len(stock_prices.columns)-2):\n",
    "    stock_prices = stock_prices.drop([str(temp_stock_prices.columns[indices[i]+1])], axis=1)\n",
    "    \n",
    "stock_prices.head()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering \n",
    "\n",
    "1. VADER \n",
    "2. Word Embedding (Word2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jj = df.copy()\n",
    "df_jj = df_jj.groupby(['Date'])['Header'].apply(lambda x: ','.join(x.astype(str))).reset_index() # combine rows with the same data\n",
    "df_jj[\"Date\"] = pd.to_datetime(df_jj[\"Date\"])\n",
    "df_jj = df_jj.sort_values(\"Date\")\n",
    "df_jj = df_jj.reset_index().drop([\"index\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\edmun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps=PorterStemmer()\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "SA=SentimentIntensityAnalyzer()\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n###for text\\nfor i in range(#):\\n  tokens=[]\\n  filtered_tokens=[]\\n  final = \" \"\\n  if(type(news.loc[i,\\'Text\\'])==float):\\n    news.loc[i,\\'filtered_text\\'] =final\\n  else:\\n    tokens.append((word_tokenize(news.loc[i,\\'Text\\'])))\\n    #filter &stem tokens\\n    for w in tokens[0]:\\n      if w not in stop_words:\\n        result=pos_tag([w])\\n        if result[0][1].startswith(\\'J\\'):\\n            tag=wordnet.ADJ\\n        elif result[0][1].startswith(\\'V\\'):\\n            tag=wordnet.VERB\\n        elif result[0][1].startswith(\\'N\\'):\\n            tag=wordnet.NOUN\\n        elif result[0][1].startswith(\\'R\\'):\\n            tag=wordnet.ADV\\n        else:\\n            tag=wordnet.NOUN\\n        filtered_tokens.append(lemmatizer.lemmatize(word=result[0][0],pos=tag))                      \\n    for j in filtered_tokens:\\n      final+=j\\n      final+= \" \"\\n    print(final)\\n    news.loc[i,\\'filtered_text\\'] =final\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words=set(stopwords.words(\"english\"))\n",
    "\n",
    "###for headers\n",
    "for i in range(df_jj.shape[0]):\n",
    "  tokens=[]\n",
    "  filtered_tokens=[]\n",
    "  final = \" \"\n",
    "  if(type(df_jj.loc[i,'Header'])==float):\n",
    "    df_jj.loc[i,'filtered_header'] =final\n",
    "  else:\n",
    "    tokens.append((word_tokenize(df_jj.loc[i,'Header'])))\n",
    "    #filter &stem tokens\n",
    "    for w in tokens[0]:\n",
    "      if w not in stop_words:\n",
    "        result=pos_tag([w])\n",
    "        if result[0][1].startswith('J'):\n",
    "            tag=wordnet.ADJ\n",
    "        elif result[0][1].startswith('V'):\n",
    "            tag=wordnet.VERB\n",
    "        elif result[0][1].startswith('N'):\n",
    "            tag=wordnet.NOUN\n",
    "        elif result[0][1].startswith('R'):\n",
    "            tag=wordnet.ADV\n",
    "        else:\n",
    "            tag=wordnet.NOUN\n",
    "        filtered_tokens.append(lemmatizer.lemmatize(word=result[0][0],pos=tag))                      \n",
    "    for j in filtered_tokens:\n",
    "      final+=j\n",
    "      final+= \" \"\n",
    "    df_jj.loc[i,'filtered_header'] =final\n",
    "\n",
    "#stop_words.add() # remove those lines at the top using this / clean the data before running\n",
    "\n",
    "\"\"\"\n",
    "###for text\n",
    "for i in range(#):\n",
    "  tokens=[]\n",
    "  filtered_tokens=[]\n",
    "  final = \" \"\n",
    "  if(type(news.loc[i,'Text'])==float):\n",
    "    news.loc[i,'filtered_text'] =final\n",
    "  else:\n",
    "    tokens.append((word_tokenize(news.loc[i,'Text'])))\n",
    "    #filter &stem tokens\n",
    "    for w in tokens[0]:\n",
    "      if w not in stop_words:\n",
    "        result=pos_tag([w])\n",
    "        if result[0][1].startswith('J'):\n",
    "            tag=wordnet.ADJ\n",
    "        elif result[0][1].startswith('V'):\n",
    "            tag=wordnet.VERB\n",
    "        elif result[0][1].startswith('N'):\n",
    "            tag=wordnet.NOUN\n",
    "        elif result[0][1].startswith('R'):\n",
    "            tag=wordnet.ADV\n",
    "        else:\n",
    "            tag=wordnet.NOUN\n",
    "        filtered_tokens.append(lemmatizer.lemmatize(word=result[0][0],pos=tag))                      \n",
    "    for j in filtered_tokens:\n",
    "      final+=j\n",
    "      final+= \" \"\n",
    "    print(final)\n",
    "    news.loc[i,'filtered_text'] =final\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###for headers\n",
    "for i in range(df_jj.shape[0]):\n",
    "    if df_jj['filtered_header'][i] == \" \":\n",
    "        pol_score=0\n",
    "    else:\n",
    "        pol_score= SA.polarity_scores(df_jj['filtered_header'][i])\n",
    "\n",
    "    df_jj.loc[i,'filtered_header_score']= pol_score.get('compound') # raw score\n",
    "    df_jj.loc[i,'filtered_header_sentiments']=0\n",
    "\n",
    "    if pol_score.get('compound')>0.1:\n",
    "        df_jj.loc[i,'filtered_header_sentiments']=1\n",
    "    elif pol_score.get('compound')<-0.1:\n",
    "        df_jj.loc[i,'filtered_header_sentiments']=-1\n",
    "        \n",
    "df_jj_final = df_jj.drop([\"Header\",\"filtered_header\",\"filtered_header_score\"],axis=1) #  remove unneccessary columns, only need the date and filtered sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Volume</th>\n",
       "      <th>response_variable</th>\n",
       "      <th>filtered_header_sentiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-12-12</td>\n",
       "      <td>113.290001</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>26374400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-12-13</td>\n",
       "      <td>113.839996</td>\n",
       "      <td>115.919998</td>\n",
       "      <td>43733800</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-12-14</td>\n",
       "      <td>115.040001</td>\n",
       "      <td>116.199997</td>\n",
       "      <td>34031800</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-12-16</td>\n",
       "      <td>116.470001</td>\n",
       "      <td>116.500000</td>\n",
       "      <td>44351100</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>115.800003</td>\n",
       "      <td>117.379997</td>\n",
       "      <td>27779400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date        Open        High    Volume  response_variable  \\\n",
       "0 2016-12-12  113.290001  115.000000  26374400                1.0   \n",
       "1 2016-12-13  113.839996  115.919998  43733800               -1.0   \n",
       "2 2016-12-14  115.040001  116.199997  34031800                1.0   \n",
       "3 2016-12-16  116.470001  116.500000  44351100               -1.0   \n",
       "4 2016-12-19  115.800003  117.379997  27779400                1.0   \n",
       "\n",
       "   filtered_header_sentiments  \n",
       "0                         0.0  \n",
       "1                         0.0  \n",
       "2                        -1.0  \n",
       "3                        -1.0  \n",
       "4                         0.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_prices = stock_prices.merge(df_jj_final,on=\"Date\")\n",
    "stock_prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding (Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ed = df.copy()\n",
    "df_ed = df_ed.drop([\"Date\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def operation_to_token(x):\n",
    "    # tokenize each headline\n",
    "    tokens = word_tokenize(x)\n",
    "    #tokens=[wd.decode('utf-8').split('/')[0] for wd in lemmatize(x)]\n",
    "    # make all lowercase\n",
    "    tokens=[token.lower() for token in tokens]\n",
    "    # remove punctuation\n",
    "    words = [word for word in tokens if (word.isalpha() and word!='s' and word!='t')]\n",
    "    # remove stop words \n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    # lemmatize\n",
    "    return words    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = df_ed[\"Header\"].apply(operation_to_token)\n",
    "all_sentences = []\n",
    "for text in new_data:\n",
    "    all_sentences.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(all_sentences, min_count=1,size=10,workers=4, window=3,sg=1) # building word embedding model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('new', 0.9988701939582825),\n",
       " ('reality', 0.9986035823822021),\n",
       " ('gadfly', 0.9985471963882446),\n",
       " ('iphone', 0.9982105493545532),\n",
       " ('tax', 0.9979653358459473),\n",
       " ('eu', 0.9978306889533997),\n",
       " ('tech', 0.9978207945823669),\n",
       " ('could', 0.9977580904960632),\n",
       " ('jobs', 0.9975736141204834),\n",
       " ('million', 0.9975494146347046)]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(x):\n",
    "    \n",
    "    length = len(x)\n",
    "    temp_vector = model[x[0]]\n",
    "    temp_vector = temp_vector.copy()\n",
    "    for i in range(1,length):\n",
    "        temp_vector+=model[x[i]]\n",
    "    return temp_vector\n",
    "    #return temp_vector/length\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndf_ed[\"tokens\"] = df_ed[\"Header\"].apply(operation_to_token)\\ndf_ed[\"embedding\"] = df_ed[\"tokens\"].apply(get_vectors)\\ndf_ed_final = pd.DataFrame(df_ed[\"embedding\"].to_list(), columns=[\\'coordinate_1\\', \\'coordinate_2\\',\\'coordinate_3\\',\\'coordinate_4\\',\\'coordinate_5\\',\\'coordinate_6\\', \\'coordinate_7\\',\\'coordinate_8\\',\\'coordinate_9\\',\\'coordinate_10\\'])\\ndf_ed_final.insert(0,\"Date\",df_ed[\"Date\"])\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean dataset, combine str with same date index\n",
    "df_ed = df.copy()\n",
    "df_ed = df_ed.groupby(['Date'])['Header'].apply(lambda x: ','.join(x.astype(str))).reset_index()\n",
    "df_ed[\"Date\"] = pd.to_datetime(df_ed[\"Date\"])\n",
    "df_ed = df_ed.sort_values(\"Date\")\n",
    "df_ed = df_ed.reset_index().drop([\"index\"], axis=1)\n",
    "\n",
    "df_ed[\"tokens\"] = df_ed[\"Header\"].apply(operation_to_token)\n",
    "df_ed[\"embedding\"] = df_ed[\"tokens\"].apply(get_vectors)\n",
    "#df_ed['check_nan'] = df_ed['embedding'].isnull()\n",
    "#df_ed = df_ed[df_ed['check_nan'] == False]\n",
    "\n",
    "df_ed_final = pd.DataFrame(df_ed[\"embedding\"].to_list(), columns=list(range(1,11)))\n",
    "df_ed_final.insert(0,\"Date\",df_ed[\"Date\"])\n",
    "\n",
    "'''\n",
    "df_ed[\"tokens\"] = df_ed[\"Header\"].apply(operation_to_token)\n",
    "df_ed[\"embedding\"] = df_ed[\"tokens\"].apply(get_vectors)\n",
    "df_ed_final = pd.DataFrame(df_ed[\"embedding\"].to_list(), columns=['coordinate_1', 'coordinate_2','coordinate_3','coordinate_4','coordinate_5','coordinate_6', 'coordinate_7','coordinate_8','coordinate_9','coordinate_10'])\n",
    "df_ed_final.insert(0,\"Date\",df_ed[\"Date\"])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Volume</th>\n",
       "      <th>response_variable</th>\n",
       "      <th>filtered_header_sentiments</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-12-12</td>\n",
       "      <td>113.290001</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>26374400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.631669</td>\n",
       "      <td>-1.135485</td>\n",
       "      <td>-0.934448</td>\n",
       "      <td>-0.548317</td>\n",
       "      <td>2.137584</td>\n",
       "      <td>0.384222</td>\n",
       "      <td>0.590332</td>\n",
       "      <td>0.197387</td>\n",
       "      <td>-1.625826</td>\n",
       "      <td>3.431614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-12-13</td>\n",
       "      <td>113.839996</td>\n",
       "      <td>115.919998</td>\n",
       "      <td>43733800</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.054861</td>\n",
       "      <td>-1.312573</td>\n",
       "      <td>-0.983634</td>\n",
       "      <td>-0.847616</td>\n",
       "      <td>2.698654</td>\n",
       "      <td>0.562702</td>\n",
       "      <td>0.820525</td>\n",
       "      <td>0.241145</td>\n",
       "      <td>-1.921839</td>\n",
       "      <td>4.282112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-12-14</td>\n",
       "      <td>115.040001</td>\n",
       "      <td>116.199997</td>\n",
       "      <td>34031800</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.252120</td>\n",
       "      <td>-1.525895</td>\n",
       "      <td>-1.139843</td>\n",
       "      <td>-0.912548</td>\n",
       "      <td>2.972388</td>\n",
       "      <td>0.542293</td>\n",
       "      <td>0.923129</td>\n",
       "      <td>0.352002</td>\n",
       "      <td>-2.092270</td>\n",
       "      <td>4.723604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-12-16</td>\n",
       "      <td>116.470001</td>\n",
       "      <td>116.500000</td>\n",
       "      <td>44351100</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.377172</td>\n",
       "      <td>-0.869220</td>\n",
       "      <td>-0.627053</td>\n",
       "      <td>-0.620866</td>\n",
       "      <td>1.923679</td>\n",
       "      <td>0.326583</td>\n",
       "      <td>0.651395</td>\n",
       "      <td>0.078150</td>\n",
       "      <td>-1.311630</td>\n",
       "      <td>2.924962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>115.800003</td>\n",
       "      <td>117.379997</td>\n",
       "      <td>27779400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.559828</td>\n",
       "      <td>-1.134874</td>\n",
       "      <td>-0.766531</td>\n",
       "      <td>-0.629012</td>\n",
       "      <td>2.249535</td>\n",
       "      <td>0.379297</td>\n",
       "      <td>0.713715</td>\n",
       "      <td>0.207526</td>\n",
       "      <td>-1.593646</td>\n",
       "      <td>3.515619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date        Open        High    Volume  response_variable  \\\n",
       "0 2016-12-12  113.290001  115.000000  26374400                1.0   \n",
       "1 2016-12-13  113.839996  115.919998  43733800               -1.0   \n",
       "2 2016-12-14  115.040001  116.199997  34031800                1.0   \n",
       "3 2016-12-16  116.470001  116.500000  44351100               -1.0   \n",
       "4 2016-12-19  115.800003  117.379997  27779400                1.0   \n",
       "\n",
       "   filtered_header_sentiments         1         2         3         4  \\\n",
       "0                         0.0 -1.631669 -1.135485 -0.934448 -0.548317   \n",
       "1                         0.0 -2.054861 -1.312573 -0.983634 -0.847616   \n",
       "2                        -1.0 -2.252120 -1.525895 -1.139843 -0.912548   \n",
       "3                        -1.0 -1.377172 -0.869220 -0.627053 -0.620866   \n",
       "4                         0.0 -1.559828 -1.134874 -0.766531 -0.629012   \n",
       "\n",
       "          5         6         7         8         9        10  \n",
       "0  2.137584  0.384222  0.590332  0.197387 -1.625826  3.431614  \n",
       "1  2.698654  0.562702  0.820525  0.241145 -1.921839  4.282112  \n",
       "2  2.972388  0.542293  0.923129  0.352002 -2.092270  4.723604  \n",
       "3  1.923679  0.326583  0.651395  0.078150 -1.311630  2.924962  \n",
       "4  2.249535  0.379297  0.713715  0.207526 -1.593646  3.515619  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_full = stock_prices.merge(df_ed_final,on=\"Date\")\n",
    "#final_df_full = stock_prices.copy()\n",
    "final_df_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Classification Model\n",
    "\n",
    "1. Support-Vector Machine\n",
    "2. Random Forest \n",
    "3. K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert regressors and response into list format \n",
    "Y = final_df_full[\"response_variable\"].tolist() # response variable \n",
    "final_df_full = final_df_full.drop([\"response_variable\",\"Date\"], axis=1)\n",
    "X = final_df_full.values.tolist() # regressor variables\n",
    "X, Y = shuffle(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support-Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Support-Vector Machine ===============\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.00      0.00      0.00        37\n",
      "         1.0       0.65      1.00      0.79        70\n",
      "\n",
      "    accuracy                           0.65       107\n",
      "   macro avg       0.33      0.50      0.40       107\n",
      "weighted avg       0.43      0.65      0.52       107\n",
      "\n",
      "Accuracy score: 0.6542056074766355\n",
      "=====================================================\n"
     ]
    }
   ],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.8)\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "scaling = MinMaxScaler(feature_range=(-1,1)).fit(x_train)\n",
    "x_train = scaling.transform(x_train)\n",
    "x_test = scaling.transform(x_test)\n",
    "# y_train = y_train.reshape(90,1)\n",
    "# y_test = y_test.reshape(10,1)\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(x_train,y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "print(\"============== Support-Vector Machine ===============\")\n",
    "#print(\"Confusion Matrix:\")\n",
    "#print(confusion_matrix(y_test, y_pred))\n",
    "#print('\\n')\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred,zero_division=0))\n",
    "print('Accuracy score: ' + str(accuracy_score(y_test,y_pred)))\n",
    "print(\"=====================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Random Forest ===================\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.57      0.57      0.57        49\n",
      "         1.0       0.64      0.64      0.64        58\n",
      "\n",
      "    accuracy                           0.61       107\n",
      "   macro avg       0.60      0.60      0.60       107\n",
      "weighted avg       0.61      0.61      0.61       107\n",
      "\n",
      "Accuracy score: 0.6074766355140186\n",
      "=====================================================\n"
     ]
    }
   ],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(x_train,y_train)\n",
    "y_pred = rfc.predict(x_test)\n",
    "temp=accuracy_score(y_test,y_pred)\n",
    "\n",
    "print(\"================== Random Forest ===================\")\n",
    "#print(\"Confusion Matrix:\")\n",
    "#print(confusion_matrix(y_test, y_pred))\n",
    "#print('\\n')\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred,zero_division=0))\n",
    "print('Accuracy score: ' + str(accuracy_score(y_test,y_pred)))\n",
    "print(\"=====================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ K-Nearest Neighbor =================\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.53      0.66      0.59        35\n",
      "         1.0       0.81      0.72      0.76        72\n",
      "\n",
      "    accuracy                           0.70       107\n",
      "   macro avg       0.67      0.69      0.68       107\n",
      "weighted avg       0.72      0.70      0.71       107\n",
      "\n",
      "Accuracy score: 0.7009345794392523\n",
      "=====================================================\n"
     ]
    }
   ],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "classifier = KNeighborsClassifier(n_neighbors=1)\n",
    "classifier.fit(x_train, y_train)\n",
    "y_pred = classifier.predict(x_test)\n",
    "temp=accuracy_score(y_test,y_pred)\n",
    "\n",
    "print(\"================ K-Nearest Neighbor =================\")\n",
    "#print(\"Confusion Matrix:\")\n",
    "#print(confusion_matrix(y_test, y_pred))\n",
    "#print('\\n')\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('Accuracy score: ' + str(accuracy_score(y_test,y_pred)))\n",
    "print(\"=====================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Logistic Regression ================\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.00      0.00      0.00        32\n",
      "         1.0       0.70      1.00      0.82        75\n",
      "\n",
      "    accuracy                           0.70       107\n",
      "   macro avg       0.35      0.50      0.41       107\n",
      "weighted avg       0.49      0.70      0.58       107\n",
      "\n",
      "Accuracy score: 0.7009345794392523\n",
      "=====================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\edmun\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "logr=LogisticRegression(solver='saga',max_iter=800,multi_class='ovr')\n",
    "logr.fit(x_train,y_train)\n",
    "y_pred = logr.predict(x_test)\n",
    "temp=accuracy_score(y_test,y_pred)\n",
    "\n",
    "print(\"================ Logistic Regression ================\")\n",
    "#print(\"Confusion Matrix:\")\n",
    "#print(confusion_matrix(y_test, y_pred))\n",
    "#print('\\n')\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "print('Accuracy score: ' + str(accuracy_score(y_test,y_pred)))\n",
    "print(\"=====================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
